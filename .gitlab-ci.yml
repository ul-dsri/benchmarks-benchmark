# SPDX-FileCopyrightText: 2024 UL Research Institutes
# SPDX-License-Identifier: Apache-2.0

stages:
  - test
  - build
  - deploy

variables:
  MKDOCS_REQUIREMENTS: -r requirements.txt

include:
  - project: buildgarden/pipelines/container
    ref: 0.3.1
    file:
      - container-docker.yml
  - project: buildgarden/pipelines/detect-secrets
    ref: 0.2.1
    file:
      - detect-secrets.yml
  - project: buildgarden/pipelines/mkdocs
    ref: 0.2.0
    file:
      - mkdocs-build.yml
  - project: buildgarden/pipelines/pages
    ref: 0.1.1
    file:
      - pages.yml
  - project: buildgarden/pipelines/s3
    ref: 0.2.0
    file:
      - s3-sync-latest.yml

container-docker:
  stage: test

mkdocs-build:
  image: registry.gitlab.com/ul-dsri/party-paper
  script:
    - apt-get update -y
    - apt-get install -y --no-install-recommends git make
    - make
    - mv site/ public/
  artifacts:
    paths:
      - public
      - docs/pdf

s3-sync-latest:
  script:
    - apk add --no-cache curl
    - curl -o /tmp/mc https://dl.min.io/client/mc/release/linux-amd64/mc
    - install /tmp/mc /usr/local/bin/mc
    - rm /tmp/mc
    - mc --version
    - mc alias set "$S3_ALIAS" "$S3_HOSTNAME" "$S3_ACCESS_KEY" "$S3_SECRET_KEY"
    - export S3_TARGET="${S3_ALIAS}/${S3_BUCKET}/papers/party-papers/"
    - export HASH_FILE="hashes.txt"

    # Download hash list or create an empty one if it doesn't exist
    - |
      if mc ls "$S3_TARGET$HASH_FILE" > /dev/null 2>&1; then
        mc cp "$S3_TARGET$HASH_FILE" .
      else
        echo "No existing hash list found. Creating a new one."
        touch $HASH_FILE
      fi

    # Keep a copy of the original hash list for comparison
    - cp $HASH_FILE ${HASH_FILE}.bak

    # Compute sha1sum for normalized PDFs and decide whether to upload
    - |-
      for file in ./docs/pdf/*_normalized.pdf; do
        # Compute the sha1 hash of the normalized PDF
        hash=$(sha1sum "$file" | awk '{print $1}')

        # Check if the hash already exists in the hash list
        if grep -q "$hash" $HASH_FILE; then
          echo "PDF '$file' already exists in S3. Skipping upload."
        else
          echo "New PDF '$file'. Uploading to S3..."

          # Remove the `_normalized` suffix from the filename for the non-normalized version
          original_file=$(echo "$file" | sed 's/_normalized\.pdf$/\.pdf/')

          # Rename the file with commit info and upload to S3
          newfile="$(basename "$original_file" | sed -e 's/\.pdf$/_'$(date +%Y-%m-%d)'_'${CI_COMMIT_SHORT_SHA}'.pdf/g')"
          cp "$original_file" "$newfile"
          mc cp "$newfile" "$S3_TARGET"

          # Add the new hash to the hash list
          echo "$hash" >> $HASH_FILE
        fi
      done

    # Check if the hash file was modified
    - |
      if ! cmp -s $HASH_FILE ${HASH_FILE}.bak; then
        echo "Hash list updated. Uploading to S3..."
        mc cp $HASH_FILE "$S3_TARGET"
      else
        echo "Hash list unchanged. Skipping upload."
      fi

    # publish index file
    - echo 'filename,url' > index.csv
    - >
      mc ls "$S3_TARGET" -r
      | awk '{print $6}'
      | sed -e 's@.*@"&","https://dl.dsri.org/papers/party-papers/&"@g' >> index.csv
    - mc cp index.csv "$S3_TARGET"
