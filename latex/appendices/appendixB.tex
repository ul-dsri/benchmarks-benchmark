\section{Appendix B. Risk Scenarios}

The following collection of Risk Scenarios provides additional information on particularly complex risks.

\subsection{\#?.?. LLM-Aligned Dataset Biases}

A safety benchmark dataset can be produced with the aid of one or more LLMs. Usage of an LLM in data production introduces several potential dataset biases that have the capacity to make certain systems under test (SUTs) over or under perform relative to their benchmark performance were LLMs not used in the production of data. The nature of this risk depends on which LLM is used and under what circumstance. These risks are present regardless of whether the LLM is used to produce new prompts from an example prompt, an LLM produces new prompts taxonomically, or an LLM translates prompts between languages. The types of LLMs are:


\begin{itemize}
\item \textbf{Open Source LLMs:} These are LLMs that are openly available for anyone to download and use without the involvement of the LLM developer. Examples include OLMo, Llama3, etc.
\item \textbf{In-house LLMs:} These are LLMs that are either trained for internal use or are fine-tuned from an open source LLM.
\item \textbf{Hosted LLMs:} These are LLMs that can be invoked, but the LLMs are not themselves available for download. GPT4, ChatGPT, Claude, and Copilot are examples.
\end{itemize}


\subsubsection{Example Scenarios}
\begin{itemize}
\item[1.] \textbf{Llama3 is used off the shelf} to generate prompts conforming to the benchmark taxonomy
    \begin{itemize}
        \item[1.] \textbf{Llama3 may then underperform} relative to other SUTs because Llama3 is safety-trained and any prompts it produces are in the residual of the Meta safety program (i.e., we only get outputs Llama3 is unable to exclude). We are accidentally adversarially sampling from Meta’s worst case so Meta performs worse.
        \item[2.] \textbf{Llama3 may overperform} relative to other SUTs because the 1.0 prompts are all in-distribution for Llama3 because they were generated by the Llama3 model.
    \end{itemize}
\item[2.] \textbf{A in-house LLM} is used to generate prompts conforming to the benchmark taxonomy
    \begin{itemize}
        \item[1.] \textbf{Data vendor’s other customers may overperform} relative to other SUTs because an LLM similarly directed for multiple customers will tend to produce similar data. Therefore, the data vendor’s customers will have an advantage.
    \end{itemize}
\item[3.] \textbf{ChatGPT is used} to produce prompts conforming to the benchmark taxonomy
    \begin{itemize}
        \item[1.] This presents the same issues as with Llama3, additionally,
        \item[2.] \textbf{GPT4 may overperform} relative to other models because OpenAI logs queries and uses their inputs/outputs to improve their safety program.
    \end{itemize}
\end{itemize}


\subsubsection{Research}
Most LLM benchmarks use LLMs at one stage or another in data production. The magnitude of biases introduced by LLM tooling in data production is not currently known. A mixture of effects may simultaneously cause a SUT to over and underperform resulting in no measurable effect. The biases may also get worse through time as SUT developers advance their safety programs with new data and data vendors.

\subsubsection{Mitigations}
This risk is solved by prohibiting the use of LLMs in data production, but it may be possible to statistically detect the problem through data analysis benefiting from additional metadata on every instance.
\begin{itemize}
\item[1.]Tag each instance with the LLM(s) used in its production.
\item[2.]Partition the sample according to whether they were produced by humans or by humans augmented with LLMs.
\item[3.]Test for performance disparities between the LLM-associated data and the human generated data.
\end{itemize}


\subsubsection{Unmitigated Disclaimer}
For benchmarks that produce prompts with the aid of LLMs, the following disclaimer is recommended to avoid inappropriate reliance on the benchmark.

\begin{center}
    \begin{tcolorbox}[colback=gray!10, colframe=black!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black]
"Test data was produced with the aid of LLMs that are also systems under test or have produced data for training systems under test. This may bias results so some LLMs over or under perform relative to one another."
    \end{tcolorbox}
\end{center}



\subsubsection{Mitigated Disclaimer}
If an impact analysis has been performed, then a disclaimer, if warranted, should be written consistent with the research findings.
