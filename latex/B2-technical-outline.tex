\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{rotating}
\usepackage{makecell}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% added config
\usepackage{amsthm}
\newtheorem{dfn}{Definition}[section]
\newcommand\bb{$B_2$ }
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\title{Benchmarks' Benchmark (\texorpdfstring{\bb}{B2}): Assessing LLM Benchmark Reliability for Decision Making}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Sean McGregor,\textsuperscript{1}
  Victor Lu,\textsuperscript{2}
  Armstrong Foundjem,\textsuperscript{3}\\
  \textbf{Aishwarya Ramasethu,}\textsuperscript{4}
  \textbf{Chris Knotz,}\textsuperscript{2}\textbf{Heather Frase}\textsuperscript{5}\\
   \\
  % examples of more authors
  % \And
  % Victor Lu \\
  % Independent \\
  % % Address \\
  % \texttt{victorjunlu@gmail.com} \\
  % \AND
  % Armstrong Foundjem \\
  % Polytechnique Montreal \\
  % % Address \\
  % \texttt{foundjem@ieee.org} \\
  % \And
  % Aishwarya Ramasethu \\
  % Prediction Guard \\
  % % Address \\
  % \texttt{aishwarya@predictionguard.com } \\
  % \And
  % Daniel Reichert \\
  % Independent \\
  % % Address \\
  % \texttt{benchmarks@reichert.io} \\
  \\
  \textsuperscript{1}UL Research Institutes \textsuperscript{2}Independent
\textsuperscript{3}Polytechnique Montreal
\\
\textsuperscript{4}Prediction Guard
\textsuperscript{5}Veraitech
}



\begin{document}

\maketitle

\begin{abstract}
Large language model (LLM) benchmarks enable system use decisions informed by LLM properties, but benchmarks may be rendered unreliable for real world decision making by a variety of risks to benchmark longevity, correctness, coverage, consistency, and intelligibility. Motivated by emerging LLM safety benchmarks, on whose scores people rely on to make decisions impacting real world safety, this work presents a benchmark for LLM benchmarks inspired by National Institute of Standards and Technology risk management processes and the field of reliability engineering. High scores indicate a reduced likelihood and/or severity of inappropriate reliance on a benchmark. A variety of benchmarks designed for real world reliability and research purposes are scored and found to vary wildly in their reliability properties.
\end{abstract}


% \section{Executive Summary}

% Reliable real world Large Language Model (LLM) benchmarks clearly state their purpose and prevent or mitigate a large number of threats to their reliability. Identifying which benchmarks are reliable is not always possible without insider information on the production and maintenance of the benchmark. The Benchmarks' Benchmark (\(B_{2}\)) provides a means of:

% \begin{itemize}
%     \item[a.] {\bf identifying which benchmarks are more reliable for real world decisions}
%     \item[b.] identifying {\bf open reliability research problems}
%     \item[c.] ensuring low cost {\bf non-reliable benchmarks do not dominate} the market
%     \item[d.] advancing the development of {\bf rigorous safety standards}
% \end{itemize}

% Through time, the operational sophistication and \(B_2\) scores of highly reliable benchmark operators are expected to increase. They are currently,

% The initial scores are the following,
% \input{absolute_risk_table}

% We are pending submission from the following benchmarks,
% \begin{table}[h!]
%   \caption{Benchmark Status}
%   \label{tab:benchmark-status}
%   \centering
%   \begin{tabular}{lccc}
%     \toprule
%     \textbf{Benchmark} & \textbf{Invited} & \textbf{Self-Assessed} & \textbf{Type} \\
%     \midrule
%     MLC 1.0    & Yes     & Pending  & Refusal/Filtering \\
%     MLC 0.5    & Yes     & Pending  & Refusal/Filtering \\
%     AIRBench   & Pending & \textemdash & Refusal/Filtering \\
%     SORRYBench & Pending & \textemdash & Refusal/Filtering \\
%     COMPL-AI   & Pending & \textemdash & Refusal/Filtering \\
%     saladbench & Pending & \textemdash & Refusal/Filtering \\
%     $\langle$Others?$>$ &  &  &  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% All LLM benchmarks are invited to submit. The absence of a \(B_2\) score for a benchmark indicates the benchmark is produced for research (i.e., not real world reliable) purposes only or the benchmark organization has chosen to not score their benchmarks.


\section{Introduction}
Benchmarks have played a central role in the rapid development of machine learning systems. New benchmarks are produced, researchers optimize models to the benchmarks, and the capabilities of AI systems advance (\cite{deng2009,srivastava2022}). While benchmarks as \textit{optimization} targets have greatly advanced the capacity for ML systems to perform useful tasks, the operational, statistical, and communication requirements for producing benchmarks influencing real world decisions differ from the requirements imposed by benchmarks used in optimization. When people rely on a benchmark produced for optimization purposes (e.g., by making a decision about what is a safe use case) they may harm themselves or others by arriving at an unsupported or incorrect conclusion.

A means of separating those benchmarks intended for research and engineering purposes (i.e., ``optimization benchmarks,'' see definition \ref{dfn:optimization_benchmark}) from benchmarks intended to express performance properties in the real world (i.e., ``decision benchmarks,'' see definition \ref{dfn:decision_benchmark}) is required before one might reasonably rely upon benchmarks to evidence decisions in the real world. This work therefore presents the benchmarks' benchmark, ``\bb,'' as a means of identifying large language model (LLM) decision benchmarks.

LLM benchmarks are already evidencing real world use decisions (\cite{rottger2024safetyprompts,bommasani2024foundation}), including informing users of the use cases the model may be capable of safely supporting, which makes their reliability of great concern -- people are likely to be harmed when misinformed. Benchmarks score highly on \bb by adopting risk management processes inspired by the National Institute of Standards and Technology (NIST) information security risk management practices (\cite{nist80030r1}). In the \bb realization of risk management, we identify \textit{failure modes} (definition \ref{dfn:failure_mode}) that are a risk to \textit{benchmark reliability} (definition \ref{dfn:benchmark_reliability}) and initialize those failure modes with a risk score from which benchmarks may affirm a growing list of mitigations. Those mitigations reduce the risk's severity or likelihood and it is that risk reduction that is summed to form the \bb score. A high \bb score indicates the scored benchmark has mitigated more of the risk to reliability and is thus more reliable for real world decision making.

\begin{dfn}
\label{dfn:optimization_benchmark}
{\bf Optimization benchmark.} A benchmark that is directly or indirectly the target of optimization for an engineering or research program. This may alternatively be defined as a ``research benchmark.''
\end{dfn}

\begin{dfn}
\label{dfn:decision_benchmark}
{\bf Decision benchmark.} A benchmark produced to inform the decision making of people exploring adoption of an LLM or its application in a particular real world scenario.
\end{dfn}

\begin{dfn}
\label{dfn:failure_mode}
{\bf Failure mode.} The way in which a system (e.g. a car, government, logistics software, benchmark, etc.) could potentially fail to operate as intended..
\end{dfn}

\begin{dfn}
\label{dfn:benchmark_reliability}
{\bf Benchmark reliability.} The ability for a benchmark to inform real-world decision-making in a stated operating context for a specified amount of time and with no failures.\footnote{ definition adapted from \cite{Rausand2004} page 1. However, the general definition of reliability predates this book by decades. \cite{ReliabilityMcLinn} with the field of Reliability engineering .}
\end{dfn}

% \begin{center}
%     \begin{tcolorbox}[colback=blue!10, colframe=blue!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black, halign=center]
%         \bf \bb is for ``Benchmark Reliability'' NOT a ``Reliability Benchmark''
%     \end{tcolorbox}
% \end{center}

% As a relatively new science, LLM benchmarking poses many unidentified modes of failure that can reduce benchmark reliability, increasing the risk of reaching inappropriate conclusions regarding the systems under test.  Therefore, we treat the identification of LLM benchmark failure mode, coupled with an approach to aggregate the failure modes into a quantified risk score, as an opportunity to advance the practice of LMM benchmarking.

The following paper introduces \bb and provides the current reliability scores of prominent benchmarks. The appendices provide the \bb failure mode and mitigation registry allowing for its easy application to new benchmarks. All LLM benchmarks are invited to self-score and submit their own scores, failure modes, and mitigations, from which we hope to collaboratively advance the practice of reliable benchmarking. The absence of a \bb score for a benchmark indicates the benchmark is produced for research (i.e., not real world reliable) purposes only or the benchmark organization has chosen to not score their benchmarks.

\begin{center}
    \begin{tcolorbox}[colback=blue!10, colframe=blue!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black, halign=left]
\textbf{Disclaimers}
\begin{itemize}
\item \bb is being actively developed and updated through time. For the most up to date paper and presentation of the benchmark, please visit \url{https://b2.dsri.org/}.
\item \bb is only as reliable as the representations made by organizations publishing the associated benchmarks.
\item \bb is intended to serve as a guide for producing and adopting best-in-class LLM benchmarks, but this work and its associated scores are not a substitute for learning more about the covered benchmarks and developing an independent sense for their reliability.
\end{itemize}
    \end{tcolorbox}
\end{center}

\section{Related work}

Hundreds of AI safety benchmarks are being used to influence decision making (\cite{rottger2024safetyprompts,bommasani2024foundation}), but the reliability of these benchmarks is being called into question as a result of developers training (intentionally or inadvertently) against benchmark datasets, the size of the datasets are too small to provide accurate measurement, the scope of the benchmarks are not clearly defined or the benchmark is being used in ways contrary to their published purpose, etc. (See \cite{raji2021ai,liao2023rethinking,mcintosh2024inadequacies,banerjee2024vulnerability,balloccu2024leak,nytimesAIMeasurement,hardy2024more,themarkupEveryoneJudging,anthropicChallengesEvaluating}, among others).

Towards improving the scientific reliability of LLM benchmarks, a variety of recent studies examine the gaps in benchmark quality giving rise to these issues. BetterBench from \cite{reuel2024betterbench} is focused on benchmark ``quality''; how well a benchmark was designed, implemented, documented and how well it will be maintained. Better Bench is designed in collaboration with users, regulators, benchmark developers, and model providers, but the authors adopt the definition of \cite{raji2021ai} for ``benchmark,'' which calls benchmarks ``...a particular combination of a dataset or sets of datasets (at least test data, sometimes also training data), and a metric, conceptualized as representing one or more specific tasks or sets of abilities, \textbf{picked up by a community of researchers as a shared framework for the comparison of methods.}'' \bb defines benchmarks differently (i.e., as a measurement informing decision making), which substantially expands the responsibilities of the benchmark developer. We are concerned with how \textit{reliable} a benchmark is; that is to say, how well the benchmark informs a decision maker of the properties intended to be measured by the benchmark's developers.

The difference may center on the selection of the benchmark user (researcher vs. LLM user) as much as it does the benchmark purpose (scientific progress vs. decision making). For a researcher to progress the science of training LLMs, their chosen benchmarks must be widely shared to support experimental replication. However, industrial zealousness for achieving ever higher scores has made benchmarking a topic of scrutiny within national daily newspapers (example: \cite{nytimesAIMeasurement}). Still, the objectives of science and decision making are only partially in tension. The audience and aim of researchers diverge in one dimension scored by \bb (longevity), but they are aligned in a further four (see table \ref{tab:dimensions}).

\begin{table}[h!]
  \caption{Benchmark Reliability Dimensions.}
  \label{tab:benchmark-reliability-dimensions}
\label{tab:dimensions}
  \centering
  \begin{tabular}{lp{10cm}}
    \toprule
    & \textbf{Question Answered} \\
    \midrule
    \textbf{Correctness} & Could the benchmark results be systematically wrong (e.g., biased) in some way? \\
    \textbf{Comprehensiveness} & Would a reasonable person believe the benchmark covers something impacting their LLM decisions that is not covered? \\
    \textbf{Consistency} & Does the score have unreasonably high variance? \\
    \textbf{Intelligibility} & Will a reasonable person understand the LLM properties as evidenced by the benchmark? \\
    \textbf{Longevity} & Does the benchmark become less correct, comprehensive, consistent, or intelligible through time (e.g., Goodhart's law \cite{strathern_improving_1997,manheim_categorizing_2019})? \\
    \bottomrule
  \end{tabular}
\end{table}

Benchmark reliability for each of these dimensions can be enhanced by the adoption of best practices introduced in other works (e.g., \cite{reuel2024betterbench,cao2025should}), but \bb's contribution is to shift the analytic frame from one of best practices, to one of quantified risks. We view existing scholarship as having identified what should be done, but the risks these best practices are addressing remain informal without evaluating a risk calculation in terms of likelihood and severity. Such a framing is required for evaluating benchmark reliability and advancing the practice of real world reliable benchmarks.

\section{Risk Assessment}
\bb's approach to assessing risk to benchmark reliability is influenced by the NIST's AI Risk Management Framework (AI RMF) \cite{nistAI_RMF} and the NIST's Guide for Conducting Risk Assessments for Information Security \cite{nist80030r1}. The AI RMF is an in-development resource for managing risks related to AI systems. Among the AI RMF documents are a collection of generative AI risks and suggested actions. Organizations may elect to associate their own in-house severity and likelihood values to each of the risks, which could then be used to score the organization across all identified risks. We have yet to encounter such a computation for AI-related risks, but this sort of analysis is more common in the practice of information security -- an older and more developed risk mitigation context.

\bb replaces the ``threats'' of information security risk assessments with the ``failure modes'' of reliability engineering. Each failure mode presents a risk of mistaken reliance.

% \begin{center}
%     \begin{tcolorbox}[colback=gray!10, colframe=black!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black]
% ``Risk assessments are used to identify, estimate, and prioritize risk to organizational operations (i.e., mission, functions, image, and reputation), organizational assets, individuals, other organizations, and the Nation, resulting from the operation and use of information systems. The purpose of risk assessments is to inform decision makers and support risk responses by identifying: (i) relevant threats to organizations or threats directed through organizations against other organizations; (ii) vulnerabilities both internal and external to organizations;(iii) impact (i.e., harm) to organizations that may occur given the potential for threats exploiting vulnerabilities; and (iv) likelihood that harm will occur. The end result is a determination of risk (i.e., typically a function of the degree of harm and likelihood of harm occurring).''\cite{nist80030r1}
%     \end{tcolorbox}
% \end{center}

Re-purposing the NIST cybersecurity guidance (\cite{nist80030r1}), \bb is produced via the following steps aggregated across all the benchmarked benchmarks.

\begin{enumerate}
    \item {\bf Identify the purpose of the assessment:} \textit{to identify and mitigate failure modes presenting risk to the reliability of the LLM benchmark}
    \item {\bf Identify the scope of the assessment:} \textit{to identify a) all failure modes likely to lead a person to make a false inference about the properties of an LLM along with b) failure mode mitigations.}
    \item {\bf Identify the assumptions and constraints associated with the assessment:} \textit{the benchmark organization will faithfully indicate the properties of their benchmark program.}
    \item {\bf Identify the sources of information to be used as inputs to the assessment:} \textit{public statements and insider knowledge about the operations of the benchmarking organization and the properties of the benchmark.}
    \item  {\bf identify the risk model and analytic approaches (i.e., assessment and analysis approaches) to be employed during the assessment.}.
\end{enumerate}

The analytic approach adopted by \bb is to identify failure modes that present risks to benchmark reliability from the ground up through the identification of failure modes and their mitigations within prominent benchmarking efforts. As a combination of techniques from several typically disjunct communities, \bb requires the statement of a large number of definitions to produce a shared understanding. These include the following.

\begin{dfn}
{\bf (Benchmark) User.} A person who makes decisions on whether to apply an LLM for a particular purpose on the basis of information conveyed by a benchmark and its associated documentation.
\end{dfn}
\begin{dfn}
{\bf System Under Test (SUT).} The system being benchmarked.
\end{dfn}
% \begin{dfn}
% {\bf Threat.} ``Any circumstance or event with the potential to adversely impact organizational operations (including mission, functions, image, or reputation), organizational assets, individuals, other organizations, or the Nation through an information system via unauthorized access, destruction, disclosure, or modification of information, and/or denial of service.'' \cite{cnssi4009}\\
% \end{dfn}
\begin{dfn}
{\bf Benchmark failure mode.} The way in which a benchmark could potentially fail to provide the User with real-world decision-making information.  An unmitigated failure mode may result in a failure that leads the User to believe that the benchmark is substantiating a claim (e.g., ``the system has been benchmarked as safe according to <name>'') that is unsubstantiated or incorrect. Adapted from \cite{cnssi4009} and \cite{Rausand2004}.
\end{dfn}
\begin{dfn}
{\bf Severity.} An assessment of the relative importance of mitigating/remediating the failure mode.  Adapted from \cite{nist80030r1}
\end{dfn}

For the purpose of \bb, we graduate severities according to the levels of table \ref{tab:severity}. \footnote{This process of describing severity rankings is a common risk assessment process across sectors. It is seen in systems reliability theory \cite{Rausand2004}. cybersecurity, and natural disasters \cite{NaturalDisasterSeverity} }
\begin{table}[!ht]
  \caption{The severity levels assigned to every failure mode to reliability in the failure mode register.}
\label{tab:severity}
  \centering
  \begin{tabular}{lp{10cm}}
    \toprule
    & \textbf{Severity Interpretation} \\
    & The identified failure mode severity may render the benchmark...\\
    \midrule
    \textbf{1.0} & completely unreliable \\
    \textbf{0.8} & unreliable \\
    \textbf{0.5} & substantially degraded in reliability  \\
    \textbf{0.2} & degraded in reliability \\
    \textbf{0.0} & non-impacted by the failure\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{dfn}
{\bf Likelihood.} A weighted factor based on a subjective estimate of the probability that a given failure mode will materialize to impact reliability. (adapted from \cite{cnssi4009} and \cite{Rausand2004}) The term likelihood ``...is not likelihood in the strict sense of the term; rather, it is a likelihood score. Risk assessors do not define a likelihood function in the statistical sense. Instead, risk assessors assign a score (or likelihood assessment) based on available evidence, experience, and expert judgment. 
\end{dfn}

For \bb, all failure modes that present a risk to a benchmark's reliability are assigned an initial likelihood of $1.0$. Benchmark creators/developers can then score points within \bb by reducing either severity or likelihood, which jointly determine ``risk.''

\begin{dfn}
{\bf Risk to benchmark reliability.} ``A composite measure of a failure mode’s probability of occurring and the magnitude or degree of the consequences of the corresponding failure.'' adapted from \cite{nist2024ai}. \bb expresses risk as $(severity*likelihood)$.
\end{dfn}

\begin{dfn}
{\bf Risk Mitigation.} ``Accepting, avoiding, mitigating, sharing, or transferring risk to organizational operations (i.e., mission, functions, image, or reputation), organizational assets, individuals, other organizations, or the Nation.'' \cite{joint2011sp} with a mitigation ``Prioritizing, evaluating, and implementing the appropriate risk-reducing controls/countermeasures recommended from the risk management process. A subset of Risk Response.'' \cite{cnssi4009}
\end{dfn}

In information security, these terms are associated with outcomes such as exposed user data, service outages, or worse outcomes. For LLM benchmarks, reliability may be impaired along several distinct dimensions.
\section{\texorpdfstring{\bb}{B2} Dimensions}
The analytic frame for ``reliability'' is broader than purely statistical properties. For safety benchmarks in particular, ``human error'' as an explanation for inappropriate decision making is not consistent with an engineering ethos that looks to prevent harm. In aviation safety, for example, when a pilot fails to safely land a plane, ``human error'' as colloquially understood dismisses a causative chain that may have been set in motion years earlier. Human error as a singular explanation of an accident obscures a series of failures in design, upkeep, and other processes leading to a bad decision as the last point of failure. Did the pilot stall the plane in mid flight, or did the avionics fail to provide a stall warning at an actionable time? Similarly, producing benchmarks such that the user makes appropriate inferences about benchmarked systems requires careful benchmark preparation and presentation to avoid inappropriate decisions. Careful production and upkeep of benchmarks respond to (i.e., mitigate) risks to benchmark reliability as categorized in Table \ref{tab:dimensions}.

These dimensions are determined by operational, statistical, and communication factors that jointly inform whether a user may rely on a benchmark when making decisions. High scores on \bb are achieved by means of mitigating or controlling risk, and indicate a reduced likelihood and/or severity of inappropriate reliance on a benchmark.

Stated formally, let $d$ be a reliability dimension within the set of reliability dimensions defined in Table \ref{tab:dimensions}. A dimension $d$ is degraded by failure mode $f$ in the set of failure modes  $F_d$. Each failure mode has a severity $f_s\in{[0,1]}$ and an assumed likelihood $f_l$ of $1.0$ prior to mitigation. Mitigation $m$ is among the set of possible adopted mitigations $M_{d,f}$ to failure mode $f$ and it reduces failure mode's likelihood by $m_l$ and severity by $m_s$. Each mitigation stacks, such if you mitigate a failure mode's likelihood by $0.5$ for two different mitigations, the resulting likelihood is $0.25$. The calculation for \bb is now given in Algorithm \ref{alg:b2}.

\begin{algorithm}
\caption{\bb for a single dimension $d$}
\label{alg:b2}
\begin{algorithmic}[1]
\State Initialize $F_d$ $\gets$ $[ \{$ failure modes to dimension d $\} ]$
\State Initialize $M_d$ $\gets$ $[ \{$ mitigation to $F_d$ \}$ ]$
\State Initialize $score \gets 0.0$
\ForAll{$f \in F_d$}
    \State $likelihood \gets 1.0$
    \State $severity \gets f_s$
    \ForAll{$m \in M_{d,t}$}
        \State $likelihood \gets likelihood - likelihood \times m_l$
        \State $severity \gets severity - severity \times m_s$
    \EndFor
    \State $score \gets score + |(likelihood\times severity) - (f_l\times f_s)|$
\EndFor
\State \Return $score$
\end{algorithmic}
\end{algorithm}

If the benchmarked benchmark scores poorly for a single dimension, it is deemed non-reliable because each dimension can singularly compromise the benchmark. The final \bb value is then reported as the minimum across all the dimensions rather than the average.

The complete set of risks and mitigations are available in Appendix A, but to present how they are rolled up into scores we will demonstrate next.
\section{Example New Failure Mode}
Here is a worked example from the perspective of a benchmark developer.

\textbf{Propose a New Failure Mode for Benchmark Reliability}

A benchmark provider identifies a failure mode that presents a risk to benchmark reliability, specifically, that ``Prompt writers [will] produce prompts with LLMs.'' The benchmark developer views this as a problem because they plan to test the LLMs utilized by the prompt writers. The failure mode is submitted to the \bb failure mode repository by opening an issue on GitHub.

\textbf{Failure Mode Acceptance}

The \bb maintainers collectively review the submitted failure mode and accept it into the next version of the \bb benchmark with the identifier \#002. Benchmarked benchmarks will be notified of the new failure mode and invited to submit their mitigations for scoring.

Benchmark developers may optionally discuss the severity of the new failure mode, but the more active maintainers of \bb propose an initial severity of 1. The severity is so high because the impact is not currently known to benchmarking science. All failure modes, including \#002, are assigned probability of 1 unless mitigated.

\textbf{Mitigation Acceptance}

Next the benchmark developers submit mitigations as identified in Table \ref{tab:reduction-measures}. The reductions in likelihood and severity are discussed and adopted via discussion on the GitHub issue.

\begin{table}[h!]
  \caption{Candidate mitigation to failure mode \#002, ``Prompt writers produce prompts with LLMs.''}
  \label{tab:reduction-measures}
  \centering
  \begin{tabular}{>{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm} p{8cm}}
    \toprule
    \textbf{Reduction in Likelihood (Percent)} & \textbf{Reduction in Severity (Percent)} & \textbf{Mitigation Measure Description} \\
    \midrule
    80 & 0 & Contractually prohibit use of LLMs in the production of test data. \\
    0 & 30 & Require formal statement of toolchain used in the production of the prompt (i.e., fully detail methods). \\
    0 & 95 & Run a study on any potentially privileged SUT via prompt generation and compare to those SUTs not involved in prompt generation. Drop LLM-generated instances if unfair advantage conferred. \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Scoring Points}

All benchmark developers participating in the next release of \bb scores affirm the mitigations that are in place for their current release version by filling out a questionnaire populated with all failures modes and mitigations.


\section{\texorpdfstring{\bb}{B2} Iterative Development}

The prior section describes the upkeep of \bb more than the initial production of the benchmark. \bb was seeded prior to public contribution iteratively, from the ground up, by processing a series of benchmarks and research papers. At each iteration, new failure modes and mitigations were identified. 

\textbf{(1) ML Commons 0.5} - LLM Product Safety Benchmark

The first benchmark scored was also the inspiration for producing \bb. Released in 2024, the ML Commons 0.5 safety benchmark \cite{vidgen2024introducing} was then a state of the art benchmark representing many of the best practices that would come to score highly on the BetterBench, however, when producing the benchmark for real world safety, the benchmark authors identified a collection of risks that would lead people to a false sense of safety. We therefore collected the list of what were then merely termed ``issues'' along with the ``fixes'' adopted, where possible.

\textbf{(2) AILuminate (ML Commons 1.0)} - LLM Product Safety Benchmark

The subsequent version of the ML Commons benchmark, ``AILuminate'' \cite{ailuminate}, then involved 100+ researchers and engineers working together to solve the previously identified failure modes along with an ever expanding list of now numbered failure modes in various states of mitigation. In both this and the prior version of the benchmark, a central artifact  communicating the properties of the benchmark was a complete taxonomy and annotation guide indicating what a tested model was expected to do and why. These expectations served to form the benchmark user's interpretation of what high scores on the benchmark mean.

\textbf{(3) BBQ} - Bias Benchmark

Have twice iterated \bb within the ML Commons working groups, we then turned to scoring the BBQ bias benchmark \cite{parrish2021bbq}. Differences in benchmark methodology highlighted where changes to failure mode descriptions were necessary and where new mitigations would need to be introduced.

\textbf{(4) BetterBench} - A benchmark quality benchmark

As a matter of literature review, we coded each of BetterBench’s (\cite{reuel2024betterbench}) questions according to whether they pertain to reliability (i.e., whether a user should rely on the benchmark for decision making) and/or scientific replication (i.e., whether a benchmark is of sufficient quality for a researcher to reproduce the benchmark results with a sampling of new data). Through this analysis, we identified five new design-related threats to reliability not identified previously within \bb along with 19 novel BetterBench best practices as mitigations to new and previously identified failure modes \bb. Figure TODO presents the results.

BetterBench also includes many mitigations required to facilitate the peer review process. Peer review is a mitigation capable of addressing many varied failure modes. Therefore we associated many of the best practices identified by BetterBench that facilitate peer review as addressing a catchall failure mode of ``Benchmark production failed to account for an idiosyncratic threat to reliability.'' This makes peer review serve the function of 'red teaming' the benchmark, which would tend to uncover novel failure modes.

\textbf{(5,6) ARC-AGI} - A benchmark for Artificial General Intelligence

Although inspired by the reliability requirements posed by safety benchmarks, we observed that those requirements could be extended to LLM benchmarking more broadly. Consequently, we did a test run of \bb on the ARC AGI (\cite{chollet2024arc}) benchmark, which measures skill acquisition efficiency as a proxy for artificial general intelligence. More interesting than its intended purpose, ARC AGI has two different versions. The public version presents as a benchmark, while a ``private'' (i.e., tightly controlled) version presents more as a competition. Interestingly, we note that the design requirements of competitions tend to produce far greater longevity properties, but those are typically expired at the conclusion of the competition. A greater study of competition practices for their mitigations would be advisable for benchmarks seeking to maximize their reliability.

\textbf{(7,8,9) TruthfulQA, AIRBench, GPQA} - Other Benchmarks

Having scored 5 benchmarks and processed one research paper, the next three benchmarks (\cite{lin2021truthfulqa,zeng2024air,rein2023gpqa}) scored produced far fewer additions to the failure mode and mitigations registry. At this point we were sufficiently confident to shift from scoring benchmarks on the basis of their publicly available materials, to asking benchmarks to self-score.

\textbf{(9) Self Scoring} - The list of submitting

We then sent the \bb questionnaire to the maintainers of the benchmarks we scored previously. ??? replied back affirming the previous results, with ??? amendments that collectively increased the scores by ??.

\textbf{(10) Future} - Continuing improvement in response to the evolving science

\bb is not a static benchmark. As new information pertaining to reliability improves our understanding of LLM benchmarking, the failure modes, mitigations, and their scores should be updated. As such, the repository hosting this paper includes a collection of issue templates for publicly submitting new failure modes, responses, and suggested amendments to these \bb components. After discussion and acceptance by the \bb maintainers, amendments and additions will be announced and a new version of \bb will become available for scoring.

\section{Results}
todo

\section{Discussion}
todo

\section{Conclusion}

LLM benchmarking is an evolving science. Benchmark developer and users are continually identifying, characterizing, and mitigating failure modes that present risk to reliability. As such, \bb is designed to track progress in developing highly-reliable benchmarks, whose adopted mitigations against failure modes for benchmark reliability provide the means for benchmark developers to increase their scores. As unknown risks and failure modes are identified along with mitigations, the maximum score increases along with the sophistication of LLM benchmarking as a reliable practice. The current list of failure modes affecting reliability and their candidate mitigations are given in Appendix B. We expect the list to grow through time.

\section{Acknowledgments}
The following people have given significant feedback to this document: Daniel Reichert, Jesse Hostetler, Rebecca Weiss, Peter Mattson, and Ryan Tovcimak.

\begin{center}
    \begin{tcolorbox}[colback=blue!10, colframe=blue!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black, halign=left]
        \textbf{Conflicts of Interest}
This research is the product of a large number of independent individuals engaged in benchmarking large language models (LLMs), including contributors to all the benchmarks herein presented. Researchers from the Digital Safety Research Institute (DSRI) of the UL Research Institutes maintain the independence of this work by ``holding the pen'' on the work. DSRI's funding arises from Underwriter's Laboratories more than 100 years of running safety testing and certification and has not received external funding for the production of this work.
\end{tcolorbox}
\end{center}


\bibliographystyle{plainnat}
\bibliography{references}

\appendix 
\input{latex/appendices/appendix_stages}
\input{appendices/appendixA.tex}
\input{appendices/appendixB.tex}
\input{appendices/appendixD.tex}

% -----------------------------------------------------------------------------------------------

\end{document}
