# Benchmarks' Benchmark (B2)

Benchmarking LLM Benchmark Practices

## About B2

Large language model (LLM) benchmarks enable system use decisions informed by LLM properties, but benchmarks may be rendered unreliable for real world decision making by a variety of threats to benchmark longevity, correctness, coverage, consistency, and intelligibility. Motivated by emerging LLM safety benchmarks, on whose scores people rely on to make decisions impacting real world safety, this work presents a benchmark for LLM benchmarks inspired by National Institute of Standards and Technology risk management processes. High scores indicate a reduced likelihood and/or severity of inappropriate reliance on a benchmark.

## B2 Results

{{ read_file('docs/data/table.md') }}

## Threat Registry
There are many threats to reliability of benchmarks.

example threat goes here

View the complete list of [threats](data/threat-registry-table.md). Submit a threat by opening a Github issue or emailing sean.mcgregor-at-ul.org.

## Mitigations
Benchmarks score points by mitigating threats to benchmark reliability.

example mitigation goes here

View the complete list of [mitigations](data/risk-response-table.md). Submit a mitigation by opening a Github issue or emailing sean.mcgregor-at-ul.org.

## Research Paper

The following are versioned releases of the research paper associated with this work. A snapshot of the paper will be submitted for peer review in May 2025.

![first page](images/first_page.png)

Current paper as of {{ current_date() }}.

Paper History:
{{ list_pdfs() }}

Contributions to the research paper are welcome via email (sean.mcgregor-at-ul.org), or on the [Github respository](https://github.com/ul-dsri/party-paper).

## Authors

- Sean McGregor ([seanbmcgregor.com](https://seanbmcgregor.com)), UL Research Institutes
- \<Your name here\> join the author list by contributing to the threat registry, mitigations, or the research paper.
- Daniel Reichert, Independent

## Disclaimer
- **Benchmark scores rely on the representations made by covered benchmarking organizations.**
- The benchmark benchmark is intended to serve as a guide for producing and adopting best-in-class LLM benchmarks, but this work and its associated scores are not a substitute for learning more about the covered benchmarks and developing an independent sense for their reliability.

## About this site
This site provides current benchmark results as self-assessed by benchmark developers.


