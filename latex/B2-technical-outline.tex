\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{rotating}
\usepackage{makecell}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% added config
\usepackage{amsthm}
\newtheorem{dfn}{Definition}[section]
\newcommand\bb{$B_2$ }
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\title{Benchmarks' Benchmark}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Sean McGregor,\textsuperscript{1}
  Victor Lu,\textsuperscript{2}
  Armstrong Foundjem,\textsuperscript{3}\\
  \textbf{Aishwarya Ramasethu,}\textsuperscript{4}
  \textbf{Daniel Reichert,}\textsuperscript{2}
  \textbf{Chris Knotz}\textsuperscript{2}\\
   \\
  % examples of more authors
  % \And
  % Victor Lu \\
  % Independent \\
  % % Address \\
  % \texttt{victorjunlu@gmail.com} \\
  % \AND
  % Armstrong Foundjem \\
  % Polytechnique Montreal \\
  % % Address \\
  % \texttt{foundjem@ieee.org} \\
  % \And
  % Aishwarya Ramasethu \\
  % Prediction Guard \\
  % % Address \\
  % \texttt{aishwarya@predictionguard.com } \\
  % \And
  % Daniel Reichert \\
  % Independent \\
  % % Address \\
  % \texttt{benchmarks@reichert.io} \\
  \\
  \textsuperscript{1}UL Research Institutes \textsuperscript{2}Independent
\textsuperscript{3}Polytechnique Montreal
\\
\textsuperscript{4}Prediction Guard
}



\begin{document}

\maketitle

\begin{abstract}
Large language model (LLM) benchmarks enable system use decisions informed by LLM properties, but benchmarks may be rendered unreliable for real world decision making by a variety of threats to benchmark longevity, correctness, coverage, consistency, and intelligibility. Motivated by emerging LLM safety benchmarks, on whose scores people rely on to make decisions impacting real world safety, this work presents a benchmark for LLM benchmarks inspired by National Institute of Standards and Technology risk management processes. High scores indicate a reduced likelihood and/or severity of inappropriate reliance on a benchmark. A variety of benchmarks designed for real world reliability and research purposes are scored and found to vary wildly in their reliability properties.
\end{abstract}


% \section{Executive Summary}

% Reliable real world Large Language Model (LLM) benchmarks clearly state their purpose and prevent or mitigate a large number of threats to their reliability. Identifying which benchmarks are reliable is not always possible without insider information on the production and maintenance of the benchmark. The Benchmarks' Benchmark (\(B_{2}\)) provides a means of:

% \begin{itemize}
%     \item[a.] {\bf identifying which benchmarks are more reliable for real world decisions}
%     \item[b.] identifying {\bf open reliability research problems}
%     \item[c.] ensuring low cost {\bf non-reliable benchmarks do not dominate} the market
%     \item[d.] advancing the development of {\bf rigorous safety standards}
% \end{itemize}

% Through time, the operational sophistication and \(B_2\) scores of highly reliable benchmark operators are expected to increase. They are currently,

% The initial scores are the following,
% \input{absolute_risk_table}

% We are pending submission from the following benchmarks,
% \begin{table}[h!]
%   \caption{Benchmark Status}
%   \label{tab:benchmark-status}
%   \centering
%   \begin{tabular}{lccc}
%     \toprule
%     \textbf{Benchmark} & \textbf{Invited} & \textbf{Self-Assessed} & \textbf{Type} \\
%     \midrule
%     MLC 1.0    & Yes     & Pending  & Refusal/Filtering \\
%     MLC 0.5    & Yes     & Pending  & Refusal/Filtering \\
%     AIRBench   & Pending & \textemdash & Refusal/Filtering \\
%     SORRYBench & Pending & \textemdash & Refusal/Filtering \\
%     COMPL-AI   & Pending & \textemdash & Refusal/Filtering \\
%     saladbench & Pending & \textemdash & Refusal/Filtering \\
%     $\langle$Others?$>$ &  &  &  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% All LLM benchmarks are invited to submit. The absence of a \(B_2\) score for a benchmark indicates the benchmark is produced for research (i.e., not real world reliable) purposes only or the benchmark organization has chosen to not score their benchmarks.



\section{Introduction}
Benchmarks have played a central role in the rapid development of machine learning systems. New benchmarks are produced, researchers optimize models to the benchmarks, and the capabilities of AI systems advance \cite{deng2009,srivastava2022}. While benchmarks as \textit{optimization} targets have greatly advanced the capacity for ML systems to perform useful tasks, the operational, statistical, and communication requirements for producing benchmarks influencing real world decisions differ from the requirements imposed by benchmarks used in optimization. People who then rely on a benchmark produced for optimization purposes (e.g., by making a decision about what is a safe use case) may harm themselves or others.

A means of separating those benchmarks intended for research and engineering purposes (i.e., "optimization benchmarks," see definition \ref{dfn:optimization_benchmark}) from benchmarks intended to express performance properties in the real world (i.e., "decision benchmarks," see definition \ref{dfn:decision_benchmark}) is required before one might reasonably rely upon benchmarks for decisions involving risk. This work therefore presents the benchmarks' benchmark, "\bb", as a means of identifying large language model (LLM) decision benchmarks. Benchmarks score highly on \bb by adopting risk management processes inspired by the National Institute of Standards and Technology (NIST) information security risk management practices (\cite{nist80030r1}). In the \bb realization of risk management, we identify threats to benchmark reliability (definition \ref{dfn:benchmark_reliability}) and invite the benchmark community to express process controls and risk mitigations (i.e., "responses" to the threats) likely to reduce the severity or likelihood of those threats materializing into inappropriate reliance. A high \bb score indicates the scored benchmark is more reliable for real world decision making.

\begin{dfn}
\label{dfn:optimization_benchmark}
{\bf Optimization benchmark.} A benchmark that is directly or indirectly the target of optimization for an engineering or research program. This may alternatively be defined as a ``research benchmark."
\end{dfn}

\begin{dfn}
\label{dfn:decision_benchmark}
{\bf Decision benchmark.} A benchmark produced to inform the decision making of people exploring adoption of an LLM or its application in a particular real world scenario.
\end{dfn}

\begin{dfn}
\label{dfn:benchmark_reliability}
{\bf Benchmark reliability.} The trustworthiness of a benchmark for informing real world decision making.
\end{dfn}

\begin{center}
    \begin{tcolorbox}[colback=blue!10, colframe=blue!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black, halign=center]
        \bf \bb is for ``Benchmark Reliability" NOT ``Reliability Benchmark"
    \end{tcolorbox}
\end{center}

As a relatively new science, LLM benchmarking poses many unquantified or unidentified risks. Therefore, we treat the identification of a new risk as an opportunity for benchmarks to further advance the practice of LLM benchmarking.

The following paper introduces \bb and provides the current scores of prominent benchmarks as self-reported by those benchmark's authors. The appendices provide the \bb registry allowing for its easy application to new benchmarks. All LLM benchmarks are invited to self-score and submit. The absence of a \bb score for a benchmark indicates the benchmark is produced for research (i.e., not real world reliable) purposes only or the benchmark organization has chosen to not score their benchmarks. 

\begin{center}
    \begin{tcolorbox}[colback=blue!10, colframe=blue!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black, halign=left]
        \textbf{Conflicts of Interest}
This research is the product of a large number of independent individuals engaged in benchmarking large language models (LLMs), including contributors to all the benchmarks herein presented. Researchers from the Digital Safety Research Institute (DSRI) of the UL Research Institutes maintain the independence of this work by ``holding the pen" on the work. DSRI's funding arises from Underwriter's Laboratories more than 100 years of running safety testing and certification and has not received external funding for the production of this work.
\end{tcolorbox}
\end{center}

\begin{center}
    \begin{tcolorbox}[colback=blue!10, colframe=blue!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black, halign=left]
\textbf{Disclaimers}
\begin{itemize}
\item \bb is being actively developed and updated through time.
\item Benchmark scores rely on the representations made by covered benchmarking organizations.
\item The benchmark benchmark is intended to serve as a guide for producing and adopting best-in-class LLM benchmarks, but this work and its associated scores are not a substitute for learning more about the covered benchmarks and developing an independent sense for their reliability.
\end{itemize}
    \end{tcolorbox}
\end{center}

\section{Related work}

todo. Talk about SafetyPrompts, contrast with BetterBench's notion of ``high quality'' that is researcher centered, talk about how human evaluation is not a panacea, and fill out this section more broadly.

SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety

https://arxiv.org/abs/2404.05399


BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices

https://arxiv.org/abs/2411.12990


Challenges in Trustworthy Human Evaluation of Chatbots

https://arxiv.org/abs/2412.04363

AI and the Everything in the Whole Wide World Benchmark

https://arxiv.org/abs/2111.15366

\section{Risk Assessment}
The AI Risk Management Framework (AI RMF) details, among other things, the distinctive risks of AI systems, including ``Underdeveloped software testing standards and inability to document AI-based practices to the standard expected of traditionally engineered software for all but the simplest of cases" and ``Difficulty in performing regular AI-based software testing, or determining what to test, since AI systems are not subject to the same controls as traditional code development" \cite{nistAI_RMF}.

We treat these challenges as opportunities to identify reliability risks and develop controls
as inspired by NIST's Guide for Conducting Risk Assessments for Information Security.

\begin{center}
    \begin{tcolorbox}[colback=gray!10, colframe=black!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black]
``Risk assessments are used to identify, estimate, and prioritize risk to organizational operations (i.e., mission, functions, image, and reputation), organizational assets, individuals, other organizations, and the Nation, resulting from the operation and use of information systems. The purpose of risk assessments is to inform decision makers and support risk responses by identifying: (i) relevant threats to organizations or threats directed through organizations against other organizations; (ii) vulnerabilities both internal and external to organizations;(iii) impact (i.e., harm) to organizations that may occur given the potential for threats exploiting vulnerabilities; and (iv) likelihood that harm will occur. The end result is a determination of risk (i.e., typically a function of the degree of harm and likelihood of harm occurring)."\cite{nist80030r1}
    \end{tcolorbox}
\end{center}

Per NIST guidance, preparing for a risk assessment involves the following steps, which scope the \bb benchmark.

\begin{enumerate}
    \item {\bf Identify the purpose of the assessment:} \textit{to identify and mitigate threats to the reliability of the LLM benchmark}
    \item {\bf Identify the scope of the assessment:} \textit{all threats likely to lead a person to make a false inference about the properties of an LLM along with their mitigations.}
    \item {\bf Identify the assumptions and constraints associated with the assessment:} \textit{the benchmark organization will faithfully indicate the properties of their benchmark program.}
    \item {\bf Identify the sources of information to be used as inputs to the assessment:} \textit{insider knowledge about the operations of the benchmarking organization and the properties of the benchmark.}
\end{enumerate}

The final step is to, (5) {\bf identify the risk model and analytic approaches (i.e., assessment and analysis approaches) to be employed during the assessment.}. The analytic approach adopted by \bb is to populate threats to reliability from the ground up through the identification of threats and their responses within prominent benchmarking efforts. As a combination of techniques from several typically disjunct communities, \bb requires the statement of a large number of definitions to produce a shared understanding. These include the following.

\begin{dfn}
{\bf (Benchmark) User.} A person who makes decisions on whether to apply an LLM for a particular purpose on the basis of information conveyed by a benchmark and its associated documentation.
\end{dfn}
\begin{dfn}
{\bf System Under Test (SUT).} The system being benchmarked.
\end{dfn}
% \begin{dfn}
% {\bf Threat.} ``Any circumstance or event with the potential to adversely impact organizational operations (including mission, functions, image, or reputation), organizational assets, individuals, other organizations, or the Nation through an information system via unauthorized access, destruction, disclosure, or modification of information, and/or denial of service." \cite{cnssi4009}\\
% \end{dfn}
\begin{dfn}
{\bf Threat to Reliability.} A threat that may lead a person to believe the benchmark is substantiating a claim (e.g., ``the system has been benchmarked as safe according to <name>") that is unsubstantiated or incorrect. Adapted from \cite{cnssi4009}.
\end{dfn}
\begin{dfn}
{\bf Severity.} An assessment of the relative importance of mitigating/remediating the [threat]. Adapted from \cite{nist80030r1}
\end{dfn}

For the purpose of \bb, we graduate severities according to the levels of table \ref{tab:severity}.
\begin{table}[!ht]
  \caption{The severity levels assigned to every threat to reliability in the threat register.}
\label{tab:severity}
  \centering
  \begin{tabular}{lp{10cm}}
    \toprule
    & \textbf{Severity Interpretation} \\
    & The identified threat severity may render the benchmark... \\
    \midrule
    \textbf{1.0} & completely unreliable \\
    \textbf{0.8} & unreliable \\
    \textbf{0.5} & substantially degraded in reliability  \\
    \textbf{0.2} & degraded in reliability \\
    \textbf{0.0} & non-impacted by the threat \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{dfn}
{\bf Likelihood.} A weighted factor based on a subjective analysis of the probability that a given threat will materialize to impact reliability. (adapted from \cite{cnssi4009}) The term likelihood ``...is not likelihood in the strict sense of the term; rather, it is a likelihood score. Risk assessors do not define a likelihood function in the statistical sense. Instead, risk assessors assign a score (or likelihood assessment) based on available evidence, experience, and expert judgment. Combinations of factors such as targeting, intent, and capability thus can be used to produce a score representing the likelihood of threat initiation; combinations of factors such as capability and vulnerability severity can be used to produce a score representing the likelihood of adverse impacts; and combinations of these scores can be used to produce an overall likelihood score." \cite{nist80030r1}
\end{dfn}

For the purposes of \bb, all threats to reliability are assigned an initial likelihood of $1.0$. Benchmark authors can then score points within \bb by reducing either severity or likelihood, which jointly determine ``risk."

\begin{dfn}
{\bf Risk.} ``A measure of the extent to which an entity is threatened by a potential circumstance or event, and is typically a function of: (i) the adverse impacts that would arise if the circumstance or event occurs; and (ii) the likelihood of occurrence" \cite{nist80030r1}. \bb expresses risk as $(severity*likelihood)$.
\end{dfn}

\begin{dfn}
{\bf Risk Response and Mitigation.} ``Accepting, avoiding, mitigating, sharing, or transferring risk to organizational operations (i.e., mission, functions, image, or reputation), organizational assets, individuals, other organizations, or the Nation." \cite{joint2011sp} with a mitigation ``Prioritizing, evaluating, and implementing the appropriate risk-reducing controls/countermeasures recommended from the risk management process. A subset of Risk Response." \cite{cnssi4009}
\end{dfn}

In information security, these terms are associated with outcomes such as exposed user data, service outages, or worse outcomes. For LLM benchmarks, reliability may be impaired along several distinct dimensions.
\section{\texorpdfstring{\bb}{B2} Dimensions}
The analytic frame for ``reliability" is broader than purely statistical properties. For safety benchmarks in particular, ``human error" as an explanation for inappropriate decision making is not consistent with an engineering ethos that looks to prevent harm. In aviation safety, for example, when a pilot fails to safely land a plane, ``human error" as colloquially understood dismisses a causative chain that may have been set in motion years earlier. Human error as a singular explanation of an accident obscures a series of failures in design, maintenance, and other processes leading to a bad decision as the last point of failure. Did the pilot stall the plane in mid flight, or did the avionics fail to provide a stall warning at an actionable time? Similarly, producing benchmarks such that the user makes appropriate inferences about benchmarked systems requires careful benchmark preparation and presentation to avoid inappropriate decisions. Careful production and maintenance of benchmarks respond to (i.e., mitigate) threats to reliability as categorized in Table \ref{tab:dimensions}.

\begin{table}[h!]
  \caption{Benchmark Reliability Dimensions. The longevity dimension requires solutions to Goodhart's law as well as practices to actively maintain the benchmark. The correctness and consistency dimensions are related to standard statistical methods and metrology. ``Comprehensiveness" is related to the coverage of the benchmark. Finally, ``intelligibility" is determined by how these dimensions are expressed to people making decisions.}
  \label{tab:benchmark-reliability-dimensions}
\label{tab:dimensions}
  \centering
  \begin{tabular}{lp{10cm}}
    \toprule
    & \textbf{Question Answered} \\
    \midrule
    \textbf{Longevity} & Is the benchmark reliable for systems under test (SUTs) produced after benchmark publication? \\
    \textbf{Correctness} & Could the scores be biased in some systematic way? \\
    \textbf{Comprehensiveness} & Would a reasonable person relying on the benchmark believe the benchmark covers a use case, context, or SUT characteristic that is not covered? \\
    \textbf{Consistency} & Does the score have unreasonably high variance for its intended purpose? \\
    \textbf{Intelligibility} & Will a reasonable person understand the SUT characteristics for use cases and contexts identified by the benchmark? \\
    \bottomrule
  \end{tabular}
\end{table}

These dimensions are determined by operational, statistical, and communication factors that jointly inform whether a user may rely on a benchmark when making decisions. High scores on \bb are achieved by means of mitigating or controlling risk, and indicate a reduced likelihood and/or severity of inappropriate reliance on a benchmark.

Stated formally, let $d$ be a reliability dimension within the set of reliability dimensions defined in Table \ref{tab:dimensions}. A dimension $d$ is degraded by threat $t$ in the set of threats $T_d$. Each threat has a severity $t_s\in{[0,1]}$ and an assumed likelihood $t_l$ of $1.0$ prior to mitigation. Mitigation $m$ is among the set of adopted responses $R_{d,t}$ to threat $t$ and it reduces threat likelihood by $m_l$ and severity by $m_s$ for the associated threat. Each response stacks, such if you mitigate threat likelihood by $0.5$ for two different mitigations, the resulting likelihood is $0.25$. The calculation for \bb is now given in \ref{alg:b2}.

\begin{algorithm}
\label{alg:b2}
\caption{\bb for a single dimension $d$}
\begin{algorithmic}[1]
\State Initialize $T_d \gets [ \{ threats to dimension d \} ]$
\State Initialize $R_d \gets [ \{ responses to T_d \} ]$
\State Initialize $score \gets 0.0$
\ForAll{$t \in T_d$}
    \State $likelihood \gets 1.0$
    \State $severity \gets t_s$
    \ForAll{$m \in R_{d,t}$}
        \State $likelihood \gets likelihood - likelihood \times m_l$
        \State $severity \gets severity - severity \times m_s$
    \EndFor
    \State $score \gets score + |(likelihood\times severity) - (t_l\times t_s)|$
\EndFor
\State \Return $score$
\end{algorithmic}
\end{algorithm}

If the benchmarked benchmark scores poorly for a single dimension, it is deemed non-reliable because each dimension can singularly compromise the benchmark. The final \bb value is then reported as the minimum across all the dimensions rather than the average.

The complete set of risks and mitigations are available in Appendix A, but to present how they are rolled up into scores we will demonstrate next.
\section{Example New Threat}
Here is a worked example from the perspective of a benchmark developer.

\textbf{Add a New Threat to Reliability}

A benchmark provider identifies a threat to benchmark reliability, specifically, that ``Prompt writers [will] produce prompts with LLMs". The benchmark developer views this as a problem because they plan to test the LLMs utilized by the prompt writers.

The threat is added to the \bb threat registry and assigned the next available threat number (\#002), along with a severity of 1.0, which is the maximum value. The severity is so high because the severity is not currently known to benchmarking science. All threats, including \#002, are assigned an assumed probability of 1. Both the probability and severity will be addressed through the development of threat responses.

A researcher develops an analysis (see Appendix B), which shows the ``correctness" of the benchmark is called into question by this threat. This means any mitigation adopted by a benchmark will accrue to the ``correctness" category.

\textbf{Response Identification}

Next the benchmark developer submits the newly proposed threat and the mitigations they identified in Table \ref{tab:reduction-measures}.

\begin{table}[h!]
  \caption{Candidate responses to threat \#002, ``Prompt writers produce prompts with LLMs."}
  \label{tab:reduction-measures}
  \centering
  \begin{tabular}{>{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm} p{8cm}}
    \toprule
    \textbf{Reduction in Likelihood (Percent)} & \textbf{Reduction in Severity (Percent)} & \textbf{Response Measure Description} \\
    \midrule
    80 & 0 & Contractually prohibit use of LLMs in the production of test data. \\
    0 & 30 & Require formal statement of toolchain used in the production of the prompt (i.e., fully detail methods). \\
    0 & 95 & Run a study on any potentially privileged SUT via prompt generation and compare to those SUTs not involved in prompt generation. Drop LLM-generated instances if unfair advantage conferred. \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Threat Acceptance}

The \bb authors collectively review the submitted threat and accept it into the next evaluation of the \bb benchmark.

\textbf{Scoring Points}

The benchmark provider is still selecting prompt vendors and decides it would be faster to contractually prohibit use of LLMs in the production of prompts than to empirically evaluate whether the use of LLMs constitutes a problem. This produces a reduction in severity of 80 percent. Some existing benchmarks that rely on LLM prompts begin a study to see whether they need to drop data.
\section{LLM Production Stages}
The initial set of threats were produced by examining each of the stages of benchmark production and both brainstorming a list of threats for each stage and capturing those threats mitigated by leading benchmarks.  LLM benchmarks are typically produced in an iterative manner as detailed in Figure \ref{fig:benchmark-production}. We briefly introduce these steps in turn below.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{image1.png}
  \caption{The chain of benchmark and assessment production follows a series of steps identified above.}
  \label{fig:benchmark-production}
\end{figure}

{\bf Step 1. Task Definition.} At this stage the benchmarking organization defines the task that the LLM is expected to perform and the desirable (or undesirable) behaviors against which it is being measured. Risk \#001, which indicates there is a disconnect between the user's understanding of the benchmark and what the benchmark is actually measuring is a risk to the intelligibility of the benchmark. This categorization implies a critique of many common benchmarks that cover a vast array of capabilities without a definite scope to the benchmark. Such benchmarks greatly advance the capabilities of LLMs through their generality, but they do not provide a means for forming a mental model of what the benchmark is indicating. Analogously, the user may know an engine's horsepower, but not know whether it is in a car or a boat.

\begin{center}
    \begin{tcolorbox}[colback=gray!10, colframe=black!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black]
        {\bf Example Risk:} \#001
        \newline
        {\bf Description:} ``Specified task does not match task performed for user"
    \end{tcolorbox}
\end{center}

{\bf Step 2. Prompt Generation.} Having defined the task the LLM is expected to perform, the next step is to produce data related to that task. Since many LLM developers work with publicly available internet data, one major risk to the correctness of a benchmark is that the benchmark uses publicly available data that is in the training set of the model. Worse yet, data vendors providing prompts consistent with a testing specification may charge the benchmarking organization for data that is publicly available and part of the training program for the benchmarked systems. This particular risk can be mitigated by searching for a select sample of test data within common datasets (e.g., \cite{commoncrawl}).

\begin{center}
    \begin{tcolorbox}[colback=gray!10, colframe=black!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black]
        {\bf Example Risk:} \#003
        \newline
        {\bf Description:} ``Prompts are collected from publicly available sources and presented as novel"
    \end{tcolorbox}
\end{center}

{\bf Step 3. Prompt Inferencing.} After producing the benchmark dataset, it is time to pipe the data through systems under test (SUTs) to get the outputs. Since many popular LLMs are closely guarded by their companies and only run for users on company-controlled hardware, benchmark prompts are often sent to SUT developers via their public APIs. If the SUT developer then accidentally or intentionally logs the prompts and brings them into their model engineering, the benchmark will no longer be reliable. The risk that the prompts will be exposed to one or more SUT developers is then the sum of the risks expressed across all benchmarked SUTs. Thus a benchmark that only benchmarks SUTs on the benchmark organization's hardware is more reliable, but likely less useful as the most important SUTs to relying persons are likely not to be covered.

\begin{center}
    \begin{tcolorbox}[colback=gray!10, colframe=black!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black]
        {\bf Example Risk:} \#020
        \newline
        {\bf Description:} ``Prompts are sent to model vendors when inferencing"
    \end{tcolorbox}
\end{center}

{\bf Step 4. Output Evaluation.} After inference the benchmark dataset, the SUT outputs are typically high dimensional (e.g., full text) and require interpretation consistent with the benchmark's purpose. Often this task is performed by an evaluator model, such as an LLM. Imagine now that an open source LLM is applied for this purpose. Any SUT developer could then place the evaluator LLM into its system chain to achieve a perfect score on the benchmark. Two mitigations are possible for this particular risk. Either the evaluator model can be kept strictly internal to the benchmark evaluation, or the machine evaluator could be replaced entirely by human effort.

\begin{center}
    \begin{tcolorbox}[colback=gray!10, colframe=black!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black]
        {\bf Example Risk:} \#023
        \newline
        {\bf Description:} ``SUT developers place evaluator within system chain"
    \end{tcolorbox}
\end{center}

{\bf Step 5. Scoring.} Having labeled each individual SUT output, the next step is to statistically aggregate the responses so they can be presented to the user in some form. One example risk at this step is that important relationships uncovered at the sample level might be hidden in the aggregate. A SUT for English, French, and Hindi might perform well for English and French, while failing spectacularly for Hindi. If the scoring function produces a simple average without propagating the Hindi failure, then the benchmark is not reliable for Hindi use decisions. Such problems can be mitigated by propagating uncertainty, confidence, and exceptions as a data structure to the presentation step.

\begin{center}
    \begin{tcolorbox}[colback=gray!10, colframe=black!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black]
        {\bf Example Risk:} \#029
        \newline
        {\bf Description:} ``Failure to propagate uncertainty or confidence from lower level measures to higher level grades"
    \end{tcolorbox}
\end{center}

{\bf Step 6. Grade Presentation.} At the presentation step, the benchmark score is rendered for consumption by the user. The most common presentation at the moment is HuggingFace.co leaderboards, which are typically up to date with the latest LLM releases. Few benchmarks are fully detailed on the HuggingFace platform (e.g., contextualizing each score with information on uncertainty). They also often lack information on what not to rely on the benchmark for. As such, there is a risk the user will not understand the scope of the benchmark as presented and make a false assumption of what an LLM may appropriately be asked to do.

\begin{center}
    \begin{tcolorbox}[colback=gray!10, colframe=black!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black]
        {\bf Example Risk:} \#033
        \newline
        {\bf Description:} ``User misunderstands the scope of the benchmark"
    \end{tcolorbox}
\end{center}

{\bf Step 7. Maintenance.} The final step of the benchmark production cycle is maintenance. Benchmark organizations today typically deliver benchmarks and move on to the next problem, but LLMs and the world they act within are constantly changing. So too must the benchmarks. Setting aside the possibility of SUT developers gaming a benchmark to achieve unrealistic scores, users themselves change in their capacities, use cases, and environments. Work establishing the ecological validity of a prompt set for processing current slang will become invalid within months as new words are introduced into prompts.

\begin{center}
    \begin{tcolorbox}[colback=gray!10, colframe=black!50, width=\textwidth, boxrule=0.5mm, sharp corners, coltext=black]
        {\bf Example Risk:} \#036
        \newline
        {\bf Description:} ``User behavior shifts through time"
    \end{tcolorbox}
\end{center}

For a user to rely upon benchmark scores, threats to the integrity of the assessment must be addressed through all stages of the benchmark's assembly.
\section{Assessed Benchmarks}
The following benchmarks have been benchmarked.

{\bf ML Commons 1.0}
\begin{itemize}
\item Brief intro to 1.0
\item Strong performing points
\item Weak performing points
\item Open Research Questions (areas requiring additional analysis)
\end{itemize}

{\bf ML Commons 0.5}
\begin{itemize}
\item Brief intro to 0.5
\item Strong performing points
\item Weak performing points
\item Open Research Questions (areas requiring additional analysis)
\end{itemize}

\section{Results}
todo

\section{Conclusion}

LLM benchmarking is an evolving science with threats to reliability being identified, characterized, and mitigated through time. As such, \bb is designed to track progress in developing highly-reliable benchmarks, whose adopted mitigations against threats to reliability provide the means for benchmark developers to increase their scores. As unknown risks are identified along with mitigations, the maximum score increases along with the sophistication of LLM benchmarking as a reliable practice. The current list of threats to reliability and their candidate responses are given in Appendix A. We expect the list to grow through time.
\section{Acknowledgments}
The following people have given significant feedback to this document: Heather Frase, Jesse Hostetler, Rebecca Weiss, Peter Mattson, and Ryan Tovcimak.


\bibliographystyle{plainnat}
\bibliography{references}

\appendix 
\input{appendices/appendixA.tex}
\input{appendices/appendixB.tex}
\input{appendices/appendixD.tex}

% -----------------------------------------------------------------------------------------------

\end{document}
