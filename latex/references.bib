@inproceedings{deng2009,
  author    = {J. Deng and W. Dong and R. Socher and L. -J. Li and Kai Li and Li Fei-Fei},
  title     = {ImageNet: A large-scale hierarchical image database},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2009},
  pages     = {248--255},
  address   = {Miami, FL, USA},
  doi       = {10.1109/CVPR.2009.5206848},
}
@article{srivastava2022,
  author = {Srivastava, Aarohi and et al.},
  title = {Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  journal = {arXiv preprint arXiv:2206.04615},
  year = {2022},
  url = {https://arxiv.org/abs/2206.04615},
}
@techreport{nist80030r1,
  author       = {{National Institute of Standards and Technology}},
  title        = {Guide for Conducting Risk Assessments},
  year         = {2012},
  institution  = {U.S. Department of Commerce},
  type         = {NIST Special Publication},
  number       = {800-30 Revision 1},
  url          = {https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-30r1.pdf},
}
@book{joint2011sp,
  title={SP 800-39. managing information security risk: Organization, mission, and information system view},
  author={Joint Task Force Transformation Initiative and others},
  year={2011},
  publisher={National Institute of Standards \& Technology}
}
@misc{nistAI_RMF,
  author       = {{National Institute of Standards and Technology}},
  title        = {Appendix B: AI Risk Management Framework (AI RMF) Knowledge Base},
  year         = {2024},
  url          = {https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B},
  note         = {Accessed: 2024-11-20},
}
@techreport{cnssi4009,
  author       = {{National Security Agency}},
  title        = {National Information Assurance Glossary (CNSSI No. 4009)},
  year         = {2015},
  institution  = {Committee on National Security Systems},
  type         = {CNSSI},
  number       = {4009},
  url          = {https://www.cnss.gov/CNSS/openDoc.cfm?a=7IdLjHOaLpmTDJGUVQgnFg%3D%3D&b=F3376A4F5EDC3855D1239EC5D7C1439FDE3D5C92FBAFC2016EDBAB96CA81370EA2A85FE78C3929CB54667693F699FF31},
}
@article{manheim2019,
  author       = {Manheim, David and Garrabrant, Scott},
  title        = {Categorizing Variants of Goodhart’s Law},
  year         = {2019},
  journal      = {arXiv preprint arXiv:1803.04585},
  url          = {http://arxiv.org/abs/1803.04585},
  archivePrefix = {arXiv},
  eprint       = {1803.04585},
  primaryClass = {cs, q-fin, stat},
}
@misc{commoncrawl,
  author       = {{Common Crawl}},
  title        = {Common Crawl: Open Repository of Web Crawl Data},
  year         = {2024},
  url          = {https://commoncrawl.org/},
  note         = {Accessed: 2024-11-20},
}
% begin not currently cited
@article{keegan2024,
  author       = {Keegan, Jon},
  title        = {Everyone Is Judging AI by These Tests. But Experts Say They’re Close to Meaningless},
  journal      = {The Markup},
  year         = {2024},
  month        = {July 17},
  url          = {https://themarkup.org/artificial-intelligence/2024/07/17/everyone-is-judging-ai-by-these-tests-but-experts-say-theyre-close-to-meaningless},
  note         = {Accessed: 2024-11-20},
}
@article{rauh2024,
  author       = {Rauh, Maribeth and Marchal, Nahema and Manzini, Arianna and Hendricks, Lisa Anne and Comanescu, Ramona and Akbulut, Canfer and Stepleton, Tom and Mateos-Garcia, Juan and Bergman, Stevie and Kay, Jackie and Griffin, Conor and Bariach, Ben and Gabriel, Iason and Rieser, Verena and Isaac, William and Weidinger, Laura},
  title        = {Gaps in the Safety Evaluation of Generative AI},
  journal      = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  year         = {2024},
  url          = {https://ojs.aaai.org/index.php/AIES/article/view/31717/33884},
  note         = {Accessed: 2024-11-20},
}
@misc{bommasani2023,
  author       = {Bommasani, Rishi and Klyman, Kevin and Zhang, Daniel and Liang, Percy},
  title        = {Do Foundation Model Providers Comply with the Draft EU AI Act?},
  year         = {2023},
  month        = {June 15},
  url          = {https://crfm.stanford.edu/2023/06/15/eu-ai-act.html},
  note         = {Accessed: 2024-11-20},
}
@article{fluri2024,
  author       = {Fluri, Lukas and Paleka, Daniel and Tram{\`e}r, Florian},
  title        = {Evaluating Superhuman Models with Consistency Checks},
  journal      = {OpenReview},
  year         = {2024},
  url          = {https://openreview.net/forum?id=LpqdquO4zW},
  note         = {Accessed: 2024-11-20},
}
@article{laurito2024,
  author       = {Laurito, Walter and Davis, Benjamin and Grietzer, Peli and Gavenčiak, Tomáš and Böhm, Ada and Kulveit, Jan},
  title        = {AI AI Bias: Large Language Models Favor Their Own Generated Content},
  journal      = {arXiv preprint arXiv:2407.12856},
  year         = {2024},
  month        = {July 9},
  url          = {https://arxiv.org/abs/2407.12856},
  note         = {Accessed: 2024-11-20},
}
@article{lin2024,
  author       = {Lin, Zilong and Cui, Jian and Liao, Xiaojing and Wang, XiaoFeng},
  title        = {Malla: Demystifying Real-world Large Language Model Integrated Malicious Services},
  journal      = {arXiv preprint arXiv:2401.03315},
  year         = {2024},
  month        = {January 6},
  url          = {https://arxiv.org/abs/2401.03315},
  note         = {Accessed: 2024-11-20},
}
@article{huang2024,
  author       = {Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and Sun, Hanchi and Liu, Zhengliang and Liu, Yixin and Wang, Yijue and Zhang, Zhikun and Vidgen, Bertie and Kailkhura, Bhavya and Xiong, Caiming and Xiao, Chaowei and Li, Chunyuan and Xing, Eric and Huang, Furong and Liu, Hao and Ji, Heng and Wang, Hongyi and Zhang, Huan and Yao, Huaxiu and Kellis, Manolis and Zitnik, Marinka and Jiang, Meng and Bansal, Mohit and Zou, James and Pei, Jian and Liu, Jian and Gao, Jianfeng and Han, Jiawei and Zhao, Jieyu and Tang, Jiliang and Wang, Jindong and Vanschoren, Joaquin and Mitchell, John and Shu, Kai and Xu, Kaidi and Chang, Kai-Wei and He, Lifang and Huang, Lifu and Backes, Michael and Gong, Neil Zhenqiang and Yu, Philip S. and Chen, Pin-Yu and Gu, Quanquan and Xu, Ran and Ying, Rex and Ji, Shuiwang and Jana, Suman and Chen, Tianlong and Liu, Tianming and Zhou, Tianyi and Wang, William and Li, Xiang and Zhang, Xiangliang and Wang, Xiao and Xie, Xing and Chen, Xun and Wang, Xuyu and Liu, Yan and Ye, Yanfang and Cao, Yinzhi and Chen, Yong and Zhao, Yue},
  title        = {TrustLLM: Trustworthiness in Large Language Models},
  journal      = {arXiv preprint arXiv:2401.05561},
  year         = {2024},
  month        = {September 30},
  url          = {https://arxiv.org/abs/2401.05561},
  note         = {Accessed: 2024-11-20},
}
@article{brundage2020,
  author       = {Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth and Maharaj, Tegan and Koh, Pang Wei and Hooker, Sara and Leung, Jade and Trask, Andrew and Bluemke, Emma and Lebensold, Jonathan and O'Keefe, Cullen and Koren, Mark and Ryffel, Théo and Rubinovitz, JB and Besiroglu, Tamay and Carugati, Federica and Clark, Jack and Eckersley, Peter and de Haas, Sarah and Johnson, Maritza and Laurie, Ben and Ingerman, Alex and Krawczuk, Igor and Askell, Amanda and Cammarota, Rosario and Lohn, Andrew and Krueger, David and Stix, Charlotte and Henderson, Peter and Graham, Logan and Prunkl, Carina and Martin, Bianca and Seger, Elizabeth and Zilberman, Noa and Ó hÉigeartaigh, Seán and Kroeger, Frens and Sastry, Girish and Kagan, Rebecca and Weller, Adrian and Tse, Brian and Barnes, Elizabeth and Dafoe, Allan and Scharre, Paul and Herbert-Voss, Ariel and Rasser, Martijn and Sodhani, Shagun and Flynn, Carrick and Gilbert, Thomas Krendl and Dyer, Lisa and Khan, Saif and Bengio, Yoshua and Anderljung, Markus},
  title        = {Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims},
  journal      = {arXiv preprint arXiv:2004.07213},
  year         = {2020},
  month        = {April 15},
  url          = {https://arxiv.org/abs/2004.07213},
  note         = {Accessed: 2024-11-20},
}
@article{thakur2024,
  author       = {Thakur, Aman Singh and Choudhary, Kartik and Ramayapally, Venkat Srinik and Vaidyanathan, Sankaran and Hupkes, Dieuwke},
  title        = {Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges},
  journal      = {arXiv preprint arXiv:2406.12624},
  year         = {2024},
  month        = {June 18},
  url          = {https://arxiv.org/abs/2406.12624},
  note         = {Accessed: 2024-11-20},
}
@article{jin2024billionaire,
  author       = {Jin, Berber},
  title        = {The 27-Year-Old Billionaire Whose Army Does AI’s Dirty Work},
  journal      = {The Wall Street Journal},
  year         = {2024},
  month        = {September 20},
  url          = {https://www.wsj.com/tech/ai/alexandr-wang-scale-ai-d7c6efd7},
  note         = {Accessed: 2024-11-20},
}
@article{edwards2024math,
  author       = {Edwards, Benj},
  title        = {New Secret Math Benchmark Stumps AI Models and PhDs Alike},
  journal      = {Ars Technica},
  year         = {2024},
  month        = {November 12},
  url          = {https://arstechnica.com/ai/2024/11/new-secret-math-benchmark-stumps-ai-models-and-phds-alike/},
  note         = {Accessed: 2024-11-20},
}
@techreport{nist2024ai,
  author       = {{National Institute of Standards and Technology}},
  title        = {NIST Artificial Intelligence Risk Management Framework},
  institution  = {U.S. Department of Commerce, National Institute of Standards and Technology},
  number       = {NIST.AI.600-1},
  year         = {2024},
  url          = {https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf},
  note         = {Accessed: 2024-11-20},
}
@article{rottger2024safetyprompts,
  title={Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety},
  author={R{\"o}ttger, Paul and Pernisi, Fabio and Vidgen, Bertie and Hovy, Dirk},
  journal={arXiv preprint arXiv:2404.05399},
  year={2024}
}

@article{ReliabilityMcLinn,
author = {McLinn, James},
year = {2011},
month = {01},
pages = {8-15},
title = {A short history of reliability},
journal = {The Journal of Reliability Information}
}

@book{Rausand2004,
  added-at = {2013-11-01T13:49:46.000+0100},
  address = {Hoboken, NJ},
  author = {Rausand, Marvin and Høyland, Arnljot},
  biburl = {https://www.bibsonomy.org/bibtex/2523c7ff2aca58859e1d7576d8998f4ad/junkerm},
  groups = {public},
  interhash = {c3f337edcd3d6cdd2d83c9f583de595e},
  intrahash = {523c7ff2aca58859e1d7576d8998f4ad},
  keywords = {},
  publisher = {Wiley-Interscience},
  timestamp = {2013-11-05T11:06:00.000+0100},
  title = {System Reliability Theory: Models, Statistical Methods and Applications},
  username = {junkerm},
  year = 2004
}


@article{NaturalDisasterSeverity, author = {Caldera, H. J. and Wirasinghe, S. C.}, title = {A universal severity classification for natural disasters}, journal = {Natural Hazards}, year = {2021}, volume = {111}, issue = {2}, pages = {1533-1573}, doi = {10.1007/s11069-021-05106-9} }


@article{hardy2024more,
  title={More than Marketing? On the Information Value of AI Benchmarks for Practitioners},
  author={Hardy, Amelia and Reuel, Anka and Meimandi, Kiana Jafari and Soder, Lisa and Griffith, Allie and Asmar, Dylan M and Koyejo, Sanmi and Bernstein, Michael S and Kochenderfer, Mykel J},
  journal={arXiv preprint arXiv:2412.05520},
  year={2024}
}
@article{cao2025should,
  title={How Should I Build A Benchmark?},
  author={Cao, Jialun and Chan, Yuk-Kit and Ling, Zixuan and Wang, Wenxuan and Li, Shuqing and Liu, Mingwei and Wang, Chaozheng and Yu, Boxi and He, Pinjia and Wang, Shuai and others},
  journal={arXiv preprint arXiv:2501.10711},
  year={2025}
}
@article{reuel2024betterbench,
  title={BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices},
  author={Reuel, Anka and Hardy, Amelia and Smith, Chandler and Lamparth, Max and Hardy, Malcolm and Kochenderfer, Mykel J},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}
@article{ren2024safetywashing,
  title={Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?},
  author={Ren, Richard and Basart, Steven and Khoja, Adam and Gatti, Alice and Phan, Long and Yin, Xuwang and Mazeika, Mantas and Pan, Alexander and Mukobi, Gabriel and Kim, Ryan H and others},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}
@article{raji2021ai,
  title={AI and the everything in the whole wide world benchmark},
  author={Raji, Inioluwa Deborah and Bender, Emily M and Paullada, Amandalynne and Denton, Emily and Hanna, Alex},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{rein2023gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={Conference on Language Modeling},
  year={2025}
}

@article{chollet2024arc,
  title={Arc prize 2024: Technical report},
  author={Chollet, Francois and Knoop, Mike and Kamradt, Gregory and Landers, Bryan},
  journal={arXiv preprint arXiv:2412.04604},
  year={2024}
}
@article{parrish2021bbq,
  title={{BBQ: A hand-built bias benchmark for question answering}},
  author={Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel R},
  journal={Findings of the Association for Computational Linguistics},
  year={2022}
}
@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}
@article{zeng2024air,
  title={Air-bench 2024: A safety benchmark based on risk categories from regulations and policies},
  author={Zeng, Yi and Yang, Yu and Zhou, Andy and Tan, Jeffrey Ziwei and Tu, Yuheng and Mai, Yifan and Klyman, Kevin and Pan, Minzhou and Jia, Ruoxi and Song, Dawn and others},
  journal={arXiv preprint arXiv:2407.17436},
  year={2024}
}
@article{vidgen2024introducing,
  title={Introducing v0. 5 of the ai safety benchmark from mlcommons},
  author={Vidgen, Bertie and Agrawal, Adarsh and Ahmed, Ahmed M and Akinwande, Victor and Al-Nuaimi, Namir and Alfaraj, Najla and Alhajjar, Elie and Aroyo, Lora and Bavalatti, Trupti and Bartolo, Max and others},
  journal={arXiv preprint arXiv:2404.12241},
  year={2024}
}

@article{bommasani2024foundation,
  title={The foundation model transparency index v1. 1: May 2024},
  author={Bommasani, Rishi and Klyman, Kevin and Kapoor, Sayash and Longpre, Shayne and Xiong, Betty and Maslej, Nestor and Liang, Percy},
  journal={arXiv preprint arXiv:2407.12929},
  year={2024}
}
@article{liao2023rethinking,
  title={Rethinking model evaluation as narrowing the socio-technical gap},
  author={Liao, Q Vera and Xiao, Ziang},
  journal={arXiv preprint arXiv:2306.03100},
  year={2023}
}

@article{mcintosh2024inadequacies,
  title={Inadequacies of large language model benchmarks in the era of generative artificial intelligence},
  author={McIntosh, Timothy R and Susnjak, Teo and Arachchilage, Nalin and Liu, Tong and Watters, Paul and Halgamuge, Malka N},
  journal={arXiv preprint arXiv:2402.09880},
  year={2024}
}

@article{banerjee2024vulnerability,
  title={The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?},
  author={Banerjee, Sourav and Agarwal, Ayushi and Singh, Eishkaran},
  journal={arXiv preprint arXiv:2412.03597},
  year={2024}
}

@misc{nytimesAIMeasurement,
	author = {Kevin Roose},
	title = {{A}.{I}. {H}as a {M}easurement {P}roblem --- nytimes.com},
	howpublished = {\url{https://www.nytimes.com/2024/04/15/technology/ai-models-measurement.html}},
	year = {2024},
	note = {[Accessed 29-01-2025]},
}
@misc{themarkupEveryoneJudging,
	author = {Jon Keegan},
	title = {{E}veryone {I}s {J}udging {A}{I} by {T}hese {T}ests. {B}ut {E}xperts {S}ay {T}hey’re {C}lose to {M}eaningless – {T}he {M}arkup --- themarkup.org},
	howpublished = {\url{https://themarkup.org/artificial-intelligence/2024/07/17/everyone-is-judging-ai-by-these-tests-but-experts-say-theyre-close-to-meaningless}},
	year = {2024},
	note = {[Accessed 29-01-2025]},
}
@misc{anthropicChallengesEvaluating,
	author = {Anthropic},
	title = {{C}hallenges in evaluating {A}{I} systems --- anthropic.com},
	howpublished = {\url{https://www.anthropic.com/research/evaluating-ai-systems}},
	year = {2024},
	note = {[Accessed 29-01-2025]},
}

@article{balloccu2024leak,
  title={Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms},
  author={Balloccu, Simone and Schmidtov{\'a}, Patr{\'\i}cia and Lango, Mateusz and Du{\v{s}}ek, Ond{\v{r}}ej},
  journal={arXiv preprint arXiv:2402.03927},
  year={2024}
}

@misc{manheim_categorizing_2019,
	title = {Categorizing {Variants} of {Goodhart}'s {Law}},
	url = {http://arxiv.org/abs/1803.04585},
	doi = {10.48550/arXiv.1803.04585},
	abstract = {There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are "(at least) four different mechanisms" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.},
	urldate = {2022-11-06},
	publisher = {arXiv},
	author = {Manheim, David and Garrabrant, Scott},
	month = feb,
	year = {2019},
	note = {arXiv:1803.04585 [cs, q-fin, stat]},
	keywords = {91E45, Computer Science - Artificial Intelligence, Quantitative Finance - General Finance, Statistics - Machine Learning},
}

@article{strathern_improving_1997,
	title = {‘{Improving} ratings’: {Audit} in the {British} {University} system},
	volume = {5},
	issn = {1474-0575, 1062-7987},
	shorttitle = {‘{Improving} ratings’},
	url = {https://archive.org/details/ImprovingRatingsAuditInTheBritishUniversitySystem/mode/2up},
	doi = {10.1002/(SICI)1234-981X(199707)5:3<305::AID-EURO184>3.0.CO;2-4},
	abstract = {This paper gives an anthropological comment on what has been called the ‘audit explosion’, the proliferation of procedures for evaluating performance. In higher education the subject of audit (in this sense) is not so much the education of the students as the institutional provision for their education. British universities, as institutions, are increasingly subject to national scrutiny for teaching, research and administrative competence. In the wake of this scrutiny comes a new cultural apparatus of expectations and technologies. While the metaphor of financial auditing points to the important values of accountability, audit does more than monitor—it has a life of its own that jeopardizes the life it audits. The runaway character of assessment practices is analysed in terms of cultural practice. Higher education is intimately bound up with the origins of such practices, and is not just the latter day target of them. © 1997 by John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3},
	urldate = {2022-11-07},
	journal = {European Review},
	author = {Strathern, Marilyn},
	month = jul,
	year = {1997},
	note = {Publisher: Cambridge University Press},
	pages = {305--321},
}

@inproceedings{mcgregor_preventing_2021,
	title = {Preventing repeated real world {AI} failures by cataloging incidents: {The} {AI} {Incident} {Database}},
	volume = {35},
	shorttitle = {Preventing repeated real world {AI} failures by cataloging incidents},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {McGregor, Sean},
	year = {2021},
	note = {Issue: 17},
	pages = {15458--15463},
}

@misc{ailuminate,
	author = {ML Commons},
	title = {AILuminate Benchmark},
	howpublished = {\url{https://ailuminate.mlcommons.org/benchmarks/}},
	year = {2024},
	note = {[Accessed 28-01-2025]},
}
